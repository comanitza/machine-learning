{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Intro\n\nHello, I am well into the process of getting proficient in PyTorch and neural networks in general but I noticed tensors where still a little \"strange\" to me, so I made this small tutorial. It helped me to get a better understanding of the issue and I hope will also help others.  \n\n## What is PyTorch\n## What is a Tensor\nA tensor is a multydimensional matrix containing data of a single data type. It of course can be just a scalar or a vector but usualy in practice is a multi dimensional array. Tensors in PyTorch are similar to python lists or numpy arrays but offer more features and are designed to work with big amounts of data and are optimised for GPUs, this makes the ideal for deep learning applications.","metadata":{}},{"cell_type":"code","source":"# these are the only two imports we need, let's import them right at the start\nimport torch\nimport numpy as np\n\n# also it's nice to print the version we are using, so let's do it here\nprint(torch.version.__version__, torch.version.cuda, np.version.version)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.860295Z","iopub.execute_input":"2023-09-23T21:30:48.860870Z","iopub.status.idle":"2023-09-23T21:30:48.870031Z","shell.execute_reply.started":"2023-09-23T21:30:48.860827Z","shell.execute_reply":"2023-09-23T21:30:48.868677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Tensor creation\nWe will start by looking at simple ways of creating PyTorch tensors, in the next section we will explore more ways of creating tensors, but this is a good place to start.","metadata":{}},{"cell_type":"code","source":"# creating a tensor with a scalar value\nt1 = torch.tensor(4)\nprint(t1)\n\n# create a tensor from a vector\nt2 = torch.tensor([1, 2, 3, 4])\nprint(t2)\n\n# creating  bidemensional array/matrix\nt3 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(t3)\n\n# now let's prin the shapes of the created tensors\n\n# the shape of the first tensor, it's just a scalar, so the shape is 1\nprint(t1.shape)\n\n# the shape of the second tensor, this is a vector of thre elements, so the shape is 1 * 3\nprint(t2.shape)\n\n# the third tensor it'a a matrix of 3 * 3 elements, so this is our shape\nprint(t3.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.873183Z","iopub.execute_input":"2023-09-23T21:30:48.874106Z","iopub.status.idle":"2023-09-23T21:30:48.888992Z","shell.execute_reply.started":"2023-09-23T21:30:48.874065Z","shell.execute_reply":"2023-09-23T21:30:48.887572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using specific devices\nWe can specify a particular device on which our tensor should run.","metadata":{}},{"cell_type":"code","source":"# this is a way in which we can set the device tu cuda if available, else default to cpu\nmyDevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# create a tensor on the specified device\nt1 = torch.ones((2, 2), device=myDevice)\nprint(t1.device)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.890679Z","iopub.execute_input":"2023-09-23T21:30:48.891124Z","iopub.status.idle":"2023-09-23T21:30:48.902005Z","shell.execute_reply.started":"2023-09-23T21:30:48.891073Z","shell.execute_reply":"2023-09-23T21:30:48.900135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensor types","metadata":{}},{"cell_type":"code","source":"# let's again create a tensor, by default when creating a tensor from a list the type of the data is long/int64\nt1 = torch.tensor([1, 2, 3])\n# we can inspect the type of the elements from the tensor like this\nprint(t1.dtype)\n\n# now let's create a tensor with the datatype float/float32\nt2 = torch.tensor([4, 5, 6], dtype=torch.float32)\nprint(t2.dtype)\n\n# let's now convert an existing tensor's data type, in our case from long/int64 to double/float64\nt3 = t1.double()\nprint(t3.dtype)\n\n# here we create three dimensional matrix of zeros\nt4 = torch.zeros(size=(6, 2, 2), dtype=torch.int64)\n\n# to wrap this section let's print the tensors we created\nprint(t1)\nprint(t2)\nprint(t3)\nprint(t4)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.904220Z","iopub.execute_input":"2023-09-23T21:30:48.904619Z","iopub.status.idle":"2023-09-23T21:30:48.919829Z","shell.execute_reply.started":"2023-09-23T21:30:48.904587Z","shell.execute_reply":"2023-09-23T21:30:48.918476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient\n\nThe gradient is used for deep learning application, here we will take a quick look on how it looks on tensors.","metadata":{}},{"cell_type":"code","source":"# let's create a tensor without mentioning expicitly that we want a gradient\nt1 = torch.rand(size=(2, 2))\nprint(t1.requires_grad)\nprint(t1)\n\n# now let us declare the same tensor but declare that we want a gradient\nt2 = torch.rand(size=(2, 2), requires_grad=True)\nprint(t2.requires_grad)\nprint(t2)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.921382Z","iopub.execute_input":"2023-09-23T21:30:48.921844Z","iopub.status.idle":"2023-09-23T21:30:48.940723Z","shell.execute_reply.started":"2023-09-23T21:30:48.921802Z","shell.execute_reply":"2023-09-23T21:30:48.939140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Special Tensor creation methods\nNow let's follow with more advanced tensor creation methods.","metadata":{}},{"cell_type":"code","source":"# let's create a tensor with just zeroes of the desiered dimensions\n# please note that these methos return tensors with float/float32 datatypes\nt1 = torch.zeros((2, 2))\nprint(t1)\n\n# now we will create a tensor filled just with ones, of the desired dimension but with the specified datatype \nt2 = torch.ones((2, 3, 4), dtype=torch.int64)\nprint(t2)\n\n# we can create a tensor with random values of the wanted dimension like in the example bellow\nt3 = torch.rand((4, 4))\nprint(t3)\n\n# using the arange function we will create a tensor\n# invoking the arange with a single parameter creates a vector tensor starting with zero (inclusive) and ending at 5 (exclusively)\nt4 = torch.arange(5)\nprint(t4)\n\n# we can provide aditional parameters to the arange method\n# invoking the method like this will create a tensor with the values [2, 4, 6, 8]\nt5 = torch.arange(start=2, end=10, step=2)\nprint(t5)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.945192Z","iopub.execute_input":"2023-09-23T21:30:48.946374Z","iopub.status.idle":"2023-09-23T21:30:48.960434Z","shell.execute_reply.started":"2023-09-23T21:30:48.946329Z","shell.execute_reply":"2023-09-23T21:30:48.958844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensor math operations","metadata":{}},{"cell_type":"markdown","source":"## Tensor integration with numpy\nPyTorch is well integrated with numpy, let's see some examples.","metadata":{}},{"cell_type":"code","source":"# we create a tensor the usual way, and then convert it to a numpy array\nt1 = torch.tensor([1, 2, 3])\narr1 = t1.numpy()\n\n# print the type of the array and the tensor\nprint(type(arr1), type(t1))\n\n# now let's create tensors from the numpy array\nt2 = torch.from_numpy(arr1)\nt3 = torch.tensor(arr1)\n\n# let's print the type\nprint(type(t2), type(t3))\n\n# and now print all the data\nprint(arr1)\nprint(t1)\nprint(t2)\nprint(t3)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.962221Z","iopub.execute_input":"2023-09-23T21:30:48.962574Z","iopub.status.idle":"2023-09-23T21:30:48.974330Z","shell.execute_reply.started":"2023-09-23T21:30:48.962545Z","shell.execute_reply":"2023-09-23T21:30:48.972946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensor concatenation\nNow we will look at ways of concatenting tensors","metadata":{}},{"cell_type":"code","source":"# we will start by creating two tensors\nt1 = torch.zeros((2, 2))\nt2 = torch.ones((2, 2))                  \nprint(t1.shape, t2.shape)\n\n# concatenate the tensors on the dimension zero\nt3 = torch.concat((t1, t2), dim=0)\nprint(t3.shape)\nprint(t3)\n\n# concatenate the tensors on the dimension one\nt4 = torch.concat((t1, t2), dim=1)\nprint(t4.shape)\nprint(t4)\n\n# now we will create two tensors based on vectors\nt5 = torch.arange(start=0, end=5)\nt6 = torch.arange(start=5, end=10)\n\n# this will concatenate the tensors on the zero dimension (horizontally)\nt7 = torch.concat((t5, t6), dim=0)\nprint(t7.shape)\nprint(t7)\n\n# as we are working with tensors based on vectors we don't have a second dimension (dim = 1)\n# so to concatenate the tensors vertically we use vstack\nt8 = torch.vstack((t5, t6))\nprint(t8.shape)\nprint(t8)             ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:48.994018Z","iopub.execute_input":"2023-09-23T21:30:48.994570Z","iopub.status.idle":"2023-09-23T21:30:49.015053Z","shell.execute_reply.started":"2023-09-23T21:30:48.994534Z","shell.execute_reply":"2023-09-23T21:30:49.012454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Indexing\nIn this section we will adress the issue of interogating and even changing a specifv value from a tenor using indexing techniques.\nWe will start with some simple indexing techniques, and the move one to some pretty fancy stuff.","metadata":{}},{"cell_type":"code","source":"# we will create matrix/bidimentional tensor\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# let's interogate the exact value from the tensot\nprint(t1[1][1])\n\n# here we set the value of the tensor at a specific index, and print the tensor to see the result\nt1[1][1] = 7\nprint(t1)\n\n# we create a tensor starting from zero to 20 with the step of 2, and adress it's indexes that are save in a predetermined list\nt2 = torch.arange(start=0, end=20, step=2)\nindicies = [1, 4, 8]\nprint(t2[indicies])\n\n# this is a fancy indexing technique, here we select the values that are less than 5, or larger than 10\nprint(t2[(t2 < 5) | (t2 > 10)])\n\n# now we will select just the even values of the tensor\nprint(t2[t2.remainder(2) == 0])\n\n# here we select the values that are more than 10,and leave them as they are, the values lower than 10 will be multiplied by 10\nprint(torch.where(t2 > 10, t2, t2 * 10))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.018012Z","iopub.execute_input":"2023-09-23T21:30:49.018912Z","iopub.status.idle":"2023-09-23T21:30:49.034679Z","shell.execute_reply.started":"2023-09-23T21:30:49.018861Z","shell.execute_reply":"2023-09-23T21:30:49.033098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensor dimensions and shape\nLet us take a quick look at the way we can inspect the shape of the tensors.","metadata":{}},{"cell_type":"code","source":"# first start by creating a tensor with random values\nt1 = torch.rand((3, 2, 2))\nprint(t1)\n\n# this tells us the shape of the tensor, in our case we have 3 dimensions (sets) of 2 by 2 matrixes\n# I like to think about it as a list of lists, so we have a list[list[list[int]]] in our case\nprint(t1.shape)\n\n# this is the numbe of top dimensions, similar to ndimension\nprint(len(t1))\n\n# this is the number of top dimensions, similat to calling len() on the tensor\nprint(t1.ndimension())\n\n# this is the total number of elements in the tensor\nprint(t1.numel())","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.036252Z","iopub.execute_input":"2023-09-23T21:30:49.037915Z","iopub.status.idle":"2023-09-23T21:30:49.049478Z","shell.execute_reply.started":"2023-09-23T21:30:49.037867Z","shell.execute_reply":"2023-09-23T21:30:49.047934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Squeeze and unsqueeze\nThe squeeze method removes all the dimensions of size one from a tensor. And unsqueeze adds new dimensions of size one on the desiered dimension for a particular tensor.","metadata":{}},{"cell_type":"code","source":"# like always let's create a tensor\nt1 = torch.tensor([[[1, 2, 3, 4]]])\nprint(t1.shape)\nprint(t1)\n\n# now let's squeeze the tensor, it will remove all dimensions that are of size one, leaving us with just the vector that has values\nt2 = t1.squeeze()\nprint(t2.shape)\nprint(t2)\n\n# here we will create a mutidimensional tensor that has some dimensions of size one\nt3 = torch.tensor([ [[ [[1, 2]], [[3, 4]] ]], [[ [[5, 6]], [[6, 7]] ]] ])\nprint(t3.shape)\nprint(t3)\n\n# applying squeeze once agin removes the dimensions of size one\nt4 = t3.squeeze()\nprint(t4.shape)\nprint(t4)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.051029Z","iopub.execute_input":"2023-09-23T21:30:49.051430Z","iopub.status.idle":"2023-09-23T21:30:49.064620Z","shell.execute_reply.started":"2023-09-23T21:30:49.051398Z","shell.execute_reply":"2023-09-23T21:30:49.063404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's move to the unsqueeze part\nt1 = torch.tensor([1, 2, 3, 4])\nprint(t1.shape)\nprint(t1)\n\n# if we unsqueeze the tensor on the dimension 1, it will add a new dimension of size 1 for each element\nt2 = t1.unsqueeze(dim=1)\nprint(t2.shape)\nprint(t2)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.067799Z","iopub.execute_input":"2023-09-23T21:30:49.069636Z","iopub.status.idle":"2023-09-23T21:30:49.081017Z","shell.execute_reply.started":"2023-09-23T21:30:49.069581Z","shell.execute_reply":"2023-09-23T21:30:49.079428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reshaping\nFor many application we need to eshape the tensors that we use, in PyTorch we can use reshape or view, both are pretty much identical but rehape is more of a \"go to\" choice. In this tutorial we will use reshape.","metadata":{}},{"cell_type":"code","source":"# as alaways we will start by creating a tensor to work with\nt1 = torch.arange(end=12)\nprint(t1)\n\n# now let's try to reshape the tensor to a 3 by 4, meaning 3 rows of 4 elements each\n# please note that we are preserving the number of elements, attempting to reshape to 3 * 3 or 2 * 7 would generate an error\nt1_3_by_4 = t1.reshape(3, 4)\nprint(t1_3_by_4)\n\n# reshaping by an unknows dimension size, unknown is represented by -1. here we are saying that we want the new tensor to be 2 by unknown\n# in this case PyTorch will automaticly pick the size of the second/unknown dimension\n# this helps with large volumns of data we we might not know the size\n# please note that the dimensions still need to add up\nt1_2_by_unknown = t1.reshape(2, -1)\nprint(t1_2_by_unknown)\n\n# here again we use the unknowsn/-1 but in our first dimension\nt1_unknown_by_3 = t1.reshape(-1, 3)\nprint(t1_unknown_by_3)\n\n# and now we get even more advanced with the unknown operator\nt1_by_3_unknown_2 = t1.reshape(3, -1, 2)\nprint(t1_by_3_unknown_2)\n\n# reshape (and view) will \"update automaticaly\"\n# this means that if we change a value of the original tensor, this chnage will be reflected also in the transformed tensors\n# this means that the reshaped tensors reference the same values/objects in memory as the original tensors\n# we set the value at index 2 to an arbitrarly value, and check that both the original tensor and a reshaped one have changed\nt1[2] = 1024\nprint(t1)\nprint(t1_by_3_unknown_2)\n\n# check that the other way is also true\n# this is the case whne bth tensors reference the same values/objects in memory\nt1_by_3_unknown_2[1][0][1] = 2048\nprint(t1)\nprint(t1_by_3_unknown_2)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.083058Z","iopub.execute_input":"2023-09-23T21:30:49.083943Z","iopub.status.idle":"2023-09-23T21:30:49.100276Z","shell.execute_reply.started":"2023-09-23T21:30:49.083896Z","shell.execute_reply":"2023-09-23T21:30:49.098694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Slicing\nSlicing allows us to take just a part of a tensor, a few examples can be seen below.","metadata":{}},{"cell_type":"code","source":"# let's create a tensor and reshape it to 4 * 3\nt1 = torch.arange(12).reshape(4, 3)\nprint(t1)\n\n# now we will take the values of the column index 1\n# we notice the result is a list, next \nt2 = t1[:, 1]\nprint(t2)\n\n# if we want the result as a column, we will reshape it to a column\nt3 = t2.reshape(-1, 1)\nprint(t3)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T21:30:49.101856Z","iopub.execute_input":"2023-09-23T21:30:49.102515Z","iopub.status.idle":"2023-09-23T21:30:49.112267Z","shell.execute_reply.started":"2023-09-23T21:30:49.102480Z","shell.execute_reply":"2023-09-23T21:30:49.110396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Permutations","metadata":{}},{"cell_type":"markdown","source":"## Finish!\nYou have made it to the finsih, please upvote :)","metadata":{}}]}